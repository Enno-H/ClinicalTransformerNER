{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A full example of how to use the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:05:40.901597Z",
     "start_time": "2021-08-18T21:05:40.899386Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will use DDI data set at https://github.com/isegura/DDICorpus\n",
    "training_root = \"./N2C2_2018/train\"\n",
    "testing_root = \"./N2C2_2018/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing using NLPpreprocessing package\n",
    "\n",
    "> if you do not want to use this package, you can see the tutorial brat2bio.ipynb or https://github.com/nlplab/brat/blob/master/tools/anntoconll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T19:46:47.761329Z",
     "start_time": "2021-08-18T19:46:46.417757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'NLPreprocessing' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# download preprocessing package NLP\n",
    "! git clone https://github.com/uf-hobi-informatics-lab/NLPreprocessing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T19:58:17.989548Z",
     "start_time": "2021-08-18T19:58:17.974345Z"
    }
   },
   "outputs": [],
   "source": [
    "# link pacakge to python path\n",
    "# import necessary functions\n",
    "import sys\n",
    "sys.path.append(\"./NLPreprocessing\")\n",
    "sys.path.append(\"./NLPreprocessing/text_process\")\n",
    "sys.path.append(\"./NLPpreprocessing/text_process/sentence_tokenization.py\")\n",
    "\n",
    "import logging\n",
    "from annotation2BIO import generate_BIO, pre_processing, read_annotation_brat, BIOdata_to_file, logger\n",
    "from sentence_tokenization import logger as logger1\n",
    "\n",
    "# change log level to error to avoid too much log information in jupyter notebook\n",
    "logger1.setLevel(logging.ERROR)\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_ids = set()\n",
    "for fn in Path(training_root).glob(\"*.ann\"):\n",
    "    train_file_ids.add(fn.stem)\n",
    "    \n",
    "test_file_ids = set()\n",
    "for fn in Path(testing_root).glob(\"*.ann\"):\n",
    "    test_file_ids.add(fn.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:54:05.158495Z",
     "start_time": "2021-08-18T20:53:56.605891Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 09:43:32,229 ERROR ['LNC', (11457, 11460), (12552, 12555)]\t('NC', 'Route', (11458, 11460)) not matched by their offsets.\n",
      "2021-11-01 09:43:35,909 ERROR ['mgMWF', (9937, 9942), (11370, 11375)]\t('MWF', 'Frequency', (9939, 9942)) not matched by their offsets.\n",
      "2021-11-01 09:43:36,301 ERROR ['uRBCs', (1134, 1139), (1222, 1227)]\t('RBCs', 'Drug', (1135, 1139)) not matched by their offsets.\n",
      "2021-11-01 09:43:40,201 ERROR ['2Lnc', (7411, 7415), (8296, 8300)]\t('nc', 'Route', (7413, 7415)) not matched by their offsets.\n",
      "2021-11-01 09:43:43,731 ERROR ['upRBCs', (945, 951), (1010, 1016)]\t('pRBCs', 'Drug', (946, 951)) not matched by their offsets.\n",
      "2021-11-01 09:43:43,852 ERROR ['/', (1835, 1836), (2048, 2049)]\t('Hypoxia', 'ADE', (1836, 1843)) not matched by their offsets.\n",
      "2021-11-01 09:43:47,514 ERROR ['LNS', (1947, 1950), (2087, 2090)]\t('NS', 'Drug', (1948, 1950)) not matched by their offsets.\n",
      "2021-11-01 09:43:49,611 ERROR ['?', (613, 614), (651, 652)]\t('reaction', 'ADE', (605, 613)) not matched by their offsets.\n",
      "2021-11-01 09:43:56,715 ERROR ['mgQAM', (8220, 8225), (8933, 8938)]\t('QAM', 'Frequency', (8222, 8225)) not matched by their offsets.\n",
      "2021-11-01 09:44:01,907 ERROR ['LNC', (7430, 7433), (8276, 8279)]\t('NC', 'Route', (7431, 7433)) not matched by their offsets.\n",
      "2021-11-01 09:44:03,242 ERROR ['UPRBC', (8789, 8794), (9552, 9557)]\t('PRBC', 'Drug', (8790, 8794)) not matched by their offsets.\n",
      "2021-11-01 09:44:05,913 ERROR ['.', (1877, 1878), (1960, 1961)]\t('narcotic', 'Drug', (1840, 1848)) not matched by their offsets.\n",
      "2021-11-01 09:44:06,149 ERROR ['UPRBC', (3562, 3567), (3951, 3956)]\t('PRBC', 'Drug', (3563, 3567)) not matched by their offsets.\n",
      "2021-11-01 09:44:10,805 ERROR ['uPRBC', (1507, 1512), (1643, 1648)]\t('PRBC', 'Drug', (1508, 1512)) not matched by their offsets.\n",
      "2021-11-01 09:44:10,807 ERROR ['uPRBC', (9535, 9540), (10582, 10587)]\t('PRBC', 'Drug', (9536, 9540)) not matched by their offsets.\n"
     ]
    }
   ],
   "source": [
    "# generate BIO from brat annotation\n",
    "train_root = Path(training_root)\n",
    "train_bio = \"./2018n2c2/bio/trains\"\n",
    "output_root = Path(train_bio)\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fid in train_file_ids:\n",
    "    txt_fn = train_root / (fid + \".txt\")\n",
    "    ann_fn = train_root / (fid + \".ann\")\n",
    "    bio_fn = output_root / (fid + \".bio.txt\")\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn)\n",
    "    e2idx, entities, rels = read_annotation_brat(ann_fn)\n",
    "    nsents, sent_bound = generate_BIO(sents, entities, file_id=fid, no_overlap=False)\n",
    "    \n",
    "    BIOdata_to_file(bio_fn, nsents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:55:30.793834Z",
     "start_time": "2021-08-18T20:55:30.103629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(272, 31)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have to split the train and dev sets\n",
    "# for transformer NER, we need to name these two datasets as train.txt and dev.txt\n",
    "train_file_ids = list(train_file_ids)\n",
    "train_ids, dev_ids = train_test_split(train_file_ids, train_size=0.9, random_state=13, shuffle=True)\n",
    "len(train_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:56:16.667473Z",
     "start_time": "2021-08-18T20:56:16.626450Z"
    }
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "merged = output_root / \"merge\" # this will the final data dir we use for training\n",
    "merged.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# train\n",
    "with open(merged / \"train.txt\", \"w\") as f:\n",
    "    for fid in train_ids:\n",
    "        f.writelines(fileinput.input(output_root / (fid + \".bio.txt\")))\n",
    "    fileinput.close()\n",
    "        \n",
    "# dev\n",
    "with open(merged /\"dev.txt\", \"w\") as f:\n",
    "    for fid in dev_ids:\n",
    "        f.writelines(fileinput.input(output_root / (fid + \".bio.txt\")))\n",
    "    fileinput.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 09:44:19,099 ERROR ['/', (9915, 9916), (10741, 10742)]\t('VOMITING', 'ADE', (9916, 9924)) not matched by their offsets.\n",
      "2021-11-01 09:44:21,621 ERROR ['LNS', (937, 940), (985, 988)]\t('NS', 'Drug', (938, 940)) not matched by their offsets.\n",
      "2021-11-01 09:44:22,874 ERROR ['uFFP', (5294, 5298), (5849, 5853)]\t('FFP', 'Drug', (5295, 5298)) not matched by their offsets.\n",
      "2021-11-01 09:44:22,875 ERROR ['uPRBC', (5352, 5357), (5917, 5922)]\t('PRBC', 'Drug', (5353, 5357)) not matched by their offsets.\n",
      "2021-11-01 09:44:24,886 ERROR ['LNS', (869, 872), (913, 916)]\t('NS', 'Route', (870, 872)) not matched by their offsets.\n",
      "2021-11-01 09:44:37,368 ERROR [',', (1881, 1882), (1963, 1964)]\t('OD', 'ADE', (1879, 1881)) not matched by their offsets.\n"
     ]
    }
   ],
   "source": [
    "test_root = Path(testing_root)\n",
    "test_gold_bio = \"./2018n2c2/bio/test_gold\"\n",
    "output_root = Path(test_gold_bio)\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fid in test_file_ids:\n",
    "    txt_fn = test_root / (fid + \".txt\")\n",
    "    ann_fn = test_root / (fid + \".ann\")\n",
    "    bio_fn = output_root / (fid + \".bio.txt\")\n",
    "\n",
    "    txt, sents = pre_processing(txt_fn)\n",
    "    e2idx, entities, rels = read_annotation_brat(ann_fn)\n",
    "    nsents, sent_bound = generate_BIO(sents, entities, file_id=fid, no_overlap=False)\n",
    "\n",
    "    BIOdata_to_file(bio_fn, nsents)\n",
    "\n",
    "# test gold\n",
    "import fileinput\n",
    "\n",
    "merged = output_root / \"merge\" # this will the final data dir we use for training\n",
    "merged.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(merged /\"test.txt\", \"w\") as f:\n",
    "    for fid in test_file_ids:\n",
    "        f.writelines(fileinput.input(output_root / (fid + \".bio.txt\")))\n",
    "    fileinput.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:01:52.395035Z",
     "start_time": "2021-08-18T21:01:31.980908Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next we will train the NER model\n",
    "\n",
    "We will just use a BERT model pre-trained on general English corpora as an example\n",
    "\n",
    "In general we need GPU to train the model, running with CPU the training will be extremely slow. \n",
    "\n",
    "To use GPU, you just need to run 'export CUDA_VISIBLE_DEVICES=0' before run the training command\n",
    "\"\"\"\n",
    "\n",
    "# -1 indicates using CPU for training; \n",
    "# 0 indicate we use the GPU with ID as 0, etc.\n",
    "! export CUDA_VISIBLE_DEVICES=0 \n",
    "\n",
    "# this is just an example, please refer to the readme for how to set hyperparameters\n",
    "! python ../src/run_transformer_ner.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model bert-base-uncased \\\n",
    "      --data_dir ./ddi/ddi_bio/merge \\\n",
    "      --new_model_dir ./new_bert_ner_model \\\n",
    "      --overwrite_model_dir \\\n",
    "      --max_seq_length 128 \\\n",
    "      --data_has_offset_information \\\n",
    "      --save_model_core \\\n",
    "      --do_train \\\n",
    "      --model_selection_scoring strict-f_score-1 \\\n",
    "      --do_lower_case \\\n",
    "      --train_batch_size 8 \\\n",
    "      --train_steps 1000 \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 20 \\\n",
    "      --gradient_accumulation_steps 1 \\\n",
    "      --do_warmup \\\n",
    "      --seed 13 \\\n",
    "      --warmup_ratio 0.1 \\\n",
    "      --max_num_checkpoints 1 \\\n",
    "      --log_file ./log.txt \\\n",
    "      --progress_bar \\\n",
    "      --early_stop 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do prediction on each test set file and format prediction as brat output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:07:04.934854Z",
     "start_time": "2021-08-18T21:06:59.577336Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "running prediction\n",
    "\n",
    "In our transformer package, we have the format conversion between bio and brat; bioc implemented\n",
    "\n",
    "you still have to convert the txt files for prediction to BIO format,\n",
    "but here you do not need to assign a real annotation label, we just use O as dummy\n",
    "\"\"\"\n",
    "\n",
    "# generate bio\n",
    "test_root = Path(testing_root)\n",
    "test_bio = \"./2018n2c2/bio/test\"\n",
    "output_root = Path(test_bio)\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fn in test_root.glob(\"*.txt\"):\n",
    "    txt_fn = fn\n",
    "    bio_fn = output_root / (fn.stem + \".bio.txt\")\n",
    "\n",
    "    txt, sents = pre_processing(txt_fn)\n",
    "    annotations = [] # here we just use an empty list for annotation so that all words will be labeled as O\n",
    "    nsents, sent_bound = generate_BIO(sents, annotations, file_id=fid, no_overlap=False)\n",
    "\n",
    "    BIOdata_to_file(bio_fn, nsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "! export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# use format 1 for BRAT, 2 for BioC, 0 as default for BIO\n",
    "# see readme for more information on parameters\n",
    "! python ../ClinicalTransformerNER/src/run_transformer_batch_prediction.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model ./new_bert_ner_model \\\n",
    "      --raw_text_dir ./N2C2_2018/test \\ # this is the dir where original text file located\n",
    "      --preprocessed_text_dir ./2018n2c2/bio/test \\# this is the dir where the BIO file located\n",
    "      --output_dir ./ddi/results \\\n",
    "      --max_seq_length 128 \\\n",
    "      --do_lower_case \\\n",
    "      --eval_batch_size 8 \\\n",
    "      --log_file ./log.txt\\\n",
    "      --do_format 1 \\\n",
    "      --do_copy \\\n",
    "      --data_has_offset_information\n",
    "\n",
    "# the bio prediction output will be generateed in ./ddi/results\n",
    "# the brat prediction output will be generateed in ./ddi/results_formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation using brat eval script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n",
    "# we have a brat_eval.py script used for evaluation of NER and relation extraction based on brat format\n",
    "\n",
    "! python ../src/eval_scripts/brat_eval.py --f1 ./ddi/DDICorpusBrat/Test/MedLine --f2 ./ddi/results_formatted_output/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}